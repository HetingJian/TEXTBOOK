# 处理词与词的节点边，先用[]方法取词
def word_word_edges(p_ij):
    word_word = []
    cols = list(p_ij.columns)
    cols = [str(w) for w in cols]  
    for w1,w2 in tqdm(combinations(cols,2),total=nCr(len(cols),2)):
        if(p_ij.loc[w1,w2] > 0):
            word_word.append((w1,w2,{"weight":p_ij.loc[w1,w2]}))
    return word_word
    
    
def nCr(n,r):
    f = math.factorial
    return int(f(n)/(f(r)*f(n-r)))
    
    
def generate_text_graph(window=10):
    datafolder="./data/"
    df = pd.read_csv(os.path.join(datafolder,"t_bbe.csv))
    
    df.drop(["id","v"],axis=1,inplace=True)
    # python常见写法，先新建，再往里添加
    df = df[["t","c","b"]]
    
    book_dict = pd.read_csv(os.path.join(datafolder,"key_english.csv"))
    book_dict = {book.lower():number for book,number in zip(book_dict["field.1"],book_dict["field"])}
    
    stopwords = list(set(nltk.corpus.stopwords.words("english")))
    
    
    # 把一个chapter的text都放在一个c列里，后面标记book,不断添加
    df_data = pd.DataFrame(columns=["c","b"])
    for book in df["b"].unique():
        dum = pd.DataFrame(columns=["c","b"])
        dum["c"] = df[df["b"]==book].groupby("c").apply(lambda x:(" ":join(x["t"])).lower())
        dum["b"] = book
        df_data = pd.concat([df_data,dum],ignore_index=True)
    del df
    # 去掉stopwords
    df_data["c"] = df_data["c"].apply(lamda x:nltk.word_tokenize(x).apply(lambda x: filter_tokens(x, stopwords))
    save_as_pickle("df_data.pkl",df_data)
    
    
    #Tfidf环节
    vectorizer = TfidfVectorize(input = "content",max_features=None,tokenizer=dummy_fun,preprocessor=dummy_fun)
    vectorizer.fit(df_data["c"])
    df_tfidf = vectorizer.transform(df_data["c"])
    df_tfidf = df_tfidf.toarray()
    vocab = vectorizer.get_feature_names()
    vocab = np.array(vocab)
    df_tfidf = pd.DataFrame(df_tfidf,columns=vocab)
    
    
    
    # 为了重新计算特征权重，并将其转化为适合分类器使用的浮点值
    # Tf表示术语频率，而Tf-idf表示术语频率乘以转制文档频率
    # tf_idf(t,d)=tf(t,d)*idf(t)
    
    # idf(t)= log((1+nd)/1+df(d,t))+1
    # nd是文档的总数，df(d,t)是包含术语t的文档数
    # 然后类似做一个softmax
    """
    counts = [[3,0,1],
              [2,0,0],
              [3,0,0],
              ...] 
    """              
    # tfidf = transformer.fit_transform(counts)
    # tfidf.toarray()
    """
    array([[0.81940995, 0.        , 0.57320793],
       [1.        , 0.        , 0.        ],
       [1.        , 0.        , 0.        ],
       [1.        , 0.        , 0.        ],
       [0.47330339, 0.88089948, 0.        ],
       [0.58149261, 0.        , 0.81355169]])
    """
    # 每行都被正则化
    
    
    # 下面开始计算PMI point_wise mutual information
    #  PMI = log2底(P(x,y)/p(x)p(y))
    #  设定滑动窗口大小，计算p(x,y),就是一个小窗口里单词是否同时出现
    names = vocab
    n_i =OrdererDict((name,0) for name in names)
    word2index = OrdererDict((name,index) for index,name in enumerate(names))
    
    occurences =np.zeros((len(names),len(names)),dtype=np.int32)
    no_windows = 0      # 记录走了多少个window
    for l in tqdm(df["c"],total = len(df_data["c"])):
        for i in range(len(l)-window):
            no_windows+=1
            d=set(l[i:(i+window)])
            # d是本次循环的小窗口，window应该是输入的数据
            for w in d:
                n_i[w]+=1
            for w1,w2 in combinations(d,2):
                i1=word2index[w1]
                i2=word2index[w2]
                occurences[i1][i2]+=1
                occurences[i2][i1]+=1
    # 注意下面的n_i是names
    p_ij = pd.DataFrame(occurences,index=names,colums=names)/no_windows
    p_i = pd.Series(n_i,index = n_i.keys())/no_windows
    # 这里暂时理解p_ij是相关性数组
    
    
    # 下面开始建图
    G =nx.Graph()
    G.add_nodes_from(df_tfidf.index)
    G.add_nodes_from(vocab)
    
    document_word = [(doc,w,{"weight":df_tfidf.loc[doc,w]})for doc in df_tfidf.index,for w in df_tfidf.columns]
    
    word_word = word_word_edges(p_ij)
    G.add_edges_from(document_word)
    G.add_edges_from(word_word)
    
    
if name=="__main__":
    generate_text_graph()
    
    
    
    
class gcn(nn.Module):
    def __init__(self,X_size,A_hat,args,bias=True):
        super(gcn,self).__init__  # 继承nn.Module的init方法
        # 把输入的A_hat变成tensor
        self.A_hat = torch.tensor(A_hat,requires_grad=False).float()
        self.weight=nn.parameter.Parameter(torch.FloatTensor(X_size,args.hidden_size_1))
    
    
    
# 下面先看text_gcn的主函数
from generate_train_test_datasets import load_pickle,save_as_pickle,generate_text_graph
from models import gcn



def load_datasets(args):
    """
    f:GCN输入的一个pytorch张量(单位矩阵)
    X：未tensor化的输入
    A_hat:邻接矩阵A的变形
    selected:用于训练的已标注的node的index
    
    """
    df_data = load_pickle("df_data.pkl")
    G = load_pickle("text_graph.pkl")

    A =nx.to_numpy_matrix(G,weight="weight")
    A =A+np.eye(G.number_of_nodes())  
    degrees = []
    for d in G.degree(weight=None):
        if d==0:
            degrees.append(0)
        else:
            degrees.append(d[1]**(-0.5))
    degrees = np.diag(degrees)
    X = np.eye(G.number_of_nodes())
    A_hat = degrees@A@degrees
    f = X

    test_idxs = []
    for b_id in df_data["b"].unique():
        dum = df_data[df_data["b"]==b_id]
        if len(dum)>=4:
            test_idxs.extend(list(np.random.choice(dum.index,size=round(args.test_ratio*len(dum),replace=False))))
    
    save_as_pickle("test_idxs.pkl",test_idxs)
    
    selected =[]
    for i in range(len(df_data)):
        if i not in test_idxs:
            selected.append(i)
    save_as_pickle("selected.pkl",selected)
    
    f_selected = f[selected];f_selected=torch.from_numpy(f_selected).float()
    labels_selected = [l for idx,l in enumerate(df_data["b"]) if idx in selected]
    
    net = gcn(X.shape[1],A_hat,args)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(net.parameters(),lr=args=lr)
    
    net.train()
    evaluation_trained = []
    for e in range(start_epoch,args.num_epochs):
        optimizer.zero_grad()
        output = net(f)
        loss = criterion(output[selected],torch.tensor(labels_selected).long()-1)
        losses_per_epoch.append(loss.item())
        loss.backward()
        optimizer.step()
        if e%50 ==0；
            net.eval()
            with torch.no_grad():
                pred_labels = net(f)
                trained_accuracy
        net.train()  
    
    
    fig = plt.figure(figsize=(13,13))
    ax = fig.add_subplot(111)
    ax.scatter([i for i range(len(losses_per_epoch))],losses_per_epoch)
    ax.set_xlabel("Epoch",fontsize=15)
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    